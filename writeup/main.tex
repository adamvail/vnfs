\documentclass[letterpaper,twocolumn,11pt]{article}
\usepackage{usenix,epsfig,endnotes,amsmath}
\usepackage{alltt}
\usepackage[margin=1in, paperwidth=8.5in, paperheight=11in]{geometry}
\usepackage{url}

\title{NFS Trace Deconstruction: 
Server Side NFS Identification and Client-Side Packet Tracing in a Virtualized Environment}
\author{Rob Jellinek and Adam Vail}
\begin{document}
\maketitle

\begin{abstract}
This paper investigates server-side methods for determining whether an NFS client is virtualized or native. We perform client-side and server-side network traces that identify two key differences that can be used as heuristics in identifying virtualized clients. First, virtualized clients using bridged networking can often be detected by the first three octets of their MAC address at the link layer, which are set to unique defaults by most common virtualization platforms. Second, virtualized clients using user-mode networking experience substantially reduced bandwidth due to processing overhead in their network stack. Clients can then be distinguished according to bandwidth profiles. Finally, we investigate the sources of the reduced user-mode networking bandwidth and identify the memory management overhead inherent in network address translation as the primary contributor to slowdown.
\end{abstract}

\section{Introduction}
This paper investigates how an NFS server can differentiate native, non-virtualized NFS clients from clients running on virtual machines. 
Ultimately, it is desirable for an NFS server to be able to identify and group all virtual machines resident on the same physical machine. Such identification and grouping would allow the server to coalesce its outgoing network traffic to the virtual machines, in essence performing VM-based redundancy elimination. This would lead to better NFS performance, and lower network utilization.

Our approach consisted of three parts. First, we searched through the NFS 4.0 RFC specification \cite{shepler2003network} to find operations that could potentially differ between the virtualized and native context, or otherwise uniquely mark the underlying platform of the client machine. Second, we performed a number of client and server-side network traces on NFS 4.0 \texttt{mount} operations and file transfers with the goal of isolating differences in the action of the NFS protocol in the virtual and native settings. Finally, we timed NFS reads of varying sizes in the virtual and native settings in order to determine if timing differences could be used to distinguish native from virtual clients. 

We learned several things from these investigations. First, we were unable to differentiate any aspect of the NFS protocol in the virtual setting from its behavior in the native setting at the application layer, either in our reading of the NFS 4.0 RFC or in our network traces. This is not surprising, as there is nothing that should inherently differ about the behavior of the application layer in a virtual machine. Thus, unless explicit signals are incorporated into the NFS protocol for the purpose of identifying a client as virtualized, nothing from the vanilla NFS protocol implementation seems to identify the platform of the client. 

However, we were able to establish two heuristics for identifying virtualized NFS clients. First, our network traces showed that although NFS application-layer semantics were indistinguishable, VM-based clients could be identified by a unique MAC address assigned by default to virtual NICs when the VM is using bridged networking. Second, our NFS read timings showed that virtualized clients using user-mode (NAT) networking experience a substantial drop in bandwidth compared to a native-client baseline. By profiling client accesses and establishing a baseline bandwidth, it is possible to use bandwidth profiles to identify a virtualized guest in a controlled environment.

The remainder of our work focused on determining why and how user-mode networking suffers such a dramatic slowdown compared to bridged networking. In particular, we traced and identified the key points in the code path of KVM+QEMU user-mode networking that cause this slowdown.

The rest of the paper is organized as follows. Section \ref{sec:NFS} describes our network trace collection and timing tests, and the corresponding heuristics for differentiating native and
virtualized NFS clients. Subsection \ref{subsec:networkIntro} introduces and compares user-mode and bridged networking, which form the basis of our heuristics for differentiating clients by MAC address (Subsection \ref{subsec:macaddrs}) and bandwidth profiles (Subsection \ref{subsec:BWprofiles}). Finally, in Section \ref{sec:trace} we examine the source of slowdown in user-mode networking in KVM+QEMU.


%******************************
%[TODO: incorporate below]
%
%Looking lower in the network stack, however, we found that the MAC addresses assigned to the virtual NICs of bridged connections took on default values under all the virtualization platforms we investigated, which would allow a server to distinguish the virtualization platform of VM-based client applications in these cases. We discuss our findings and the implications in more detail in Subsection \ref{subsec:macaddrs} below. Finally, we found that NFS reads from a KVM+QEMU VM using NAT networking and virtio PCI paravirtualized network driver took an average of 72\% longer than reads from a native-hardware client, while NFS reads from a KVM+QEMU VM using bridged networking with the virtio driver took an average of 26\% longer than the client running on the same underlying native-hardware setup. Our data collection and analyses are discussed below in Section \ref{sec:NFSTiming}. 
%******************************

%TODO: change section title
\section{Differentiating Native and Virtualized NFS Clients}
\label{sec:NFS}

\subsection{Experimental Setup}
\label{subsec:expSetup}
The traces and timing experiments discussed below were run on a simple two-node topology connected by a Cisco E-2000 gigabit router. The client was a Thinkpad T60 with a dual-core 1.83GHz Intel T2400 CPU, 2GB RAM, and an Intel 82573L gigabit NIC. The server was an AMD Opteron 148 operating at 1GHz, with 1GB of RAM and a gigabit on-motherboard NIC. The client and server both used ext4 as their primary filesystem, and the client KVM+QEMU guest VM used ext4 on an 8GB QCOW2 virtual disk. The client was running Ubuntu 12.04LTS natively and in its guest VMs, and the server was running CentOS 5.2.

The network traces we collected were executed with the flags \texttt{tcpdump -vvnnXSs 1536} in order to examine the full contents of each NFS packet exchanged between the client and server.

\subsection{User-mode/Bridged Networking}
\label{subsec:networkIntro}
Both of the heuristics we discuss below rely on key differences in network behavior and timing caused by the two main virtual networking configurations used by guest VMs. Guest VMs can be configured to run either user-mode, or bridged networking.

User-mode networking performs network address translation (NAT) behind the host, which isolates guest VMs from the outside network. That is, no entities outside the physical host's network can see the guest VMs, and so without incorporating port-forwarding from the host, those guests cannot be contacted from the outside. Importantly, this also has the effect that all guest VMs share a common IP and MAC address with the host; when the guest has an established TCP session open and the host receives packets destined for that guest, the host demultiplexes those packets and forwards them to the correct guest. 

KVM+QEMU uses the SLiRP protocol \cite{slirp} to perform network address translation. SLiRP was originally used to emulate TCP/IP and provide transport-layer functionality for users with dial-up connections. The emulation code has been incorporated into the main branch of KVM+QEMU, and has been adapted to provide support for user-mode networking.

Bridged networking, on the other hand, uses a TAP device that is configured on the host. Both the physical NIC and the TAP device must be connected to a virtual network bridge on the host. Creating the TAP device requires root access, and so is not an option for all users. Guest VMs attach to the TAP device through their virtual interfaces and participate as fully fledged members of the network. This means that their virtual NICs are assigned unique IP addresses that are visible to the outside network, as well as MAC addresses that must be unique within the network.

\subsection{Identifying Bridged Hosts by MAC Address}
\label{subsec:macaddrs}
Since under virtual bridged networking the guest is assigned its own IP and MAC address and appears as a separate host on the network, it must get its IP and MAC address from some source. Typically, it is assigned its IP via DHCP or static assignment in a similar manner to the physical host. But its MAC address must first be assigned by some other means. Here, we can take advantage of the behavior of most virtualization platforms, which assign a default MAC address to the guest's virtual NIC unless the user or administrator actively sets the MAC address. The first three octets of each default hypervisor-assigned 48-bit MAC address are fixed according to the virtualization platform, which allows us to use the MAC address to identify the client's underlying platform. This MAC address is visible to the server in the MAC address fields of the Ethernet headers exchanged with the client. See Table \ref{tab:macaddrs} for a list of default MAC addresses assigned to virtual NICs under common virtualization platforms. 

This helps us distinguish virtual from native clients to some extent, but still leaves a lot to be desired if we wish to identify and \emph{group} VM-based NFS clients. First, in the bridged context, it is possible for a client VM user to specify their MAC address to be anything they desire. These defaults are merely that: default settings allocated by the virtualization manager or hypervisor if the VM user or administrator does not specify an alternative address. So there is certainly a chance for false negatives, where virtual machines using bridged networking on a certain platform cannot be identified by their MAC address because the default MAC address is not used. Second, the identification is overly broad: the NFS server can detect that it is talking with a KVM+QEMU or VMWare guest VM, for example, but it is not possible to determine that the guest VM resides on a given physical machine based on this information alone. Finally, this approach works only for guests using bridged networking, since NATed guests use the IP and MAC address of their underlying physical host.

\begin{table*}[htb]
		\centering
		\begin{tabular}{|l|l|}
		\hline
			Virtualization Platform & MAC \\
		\hline
		\hline
			KVM+QEMU & 52:54:00 \\
			VMware ESX 3, Server, Workstation, Player	& 00-50-56, 00-0C-29, 00-05-69 \\
			Microsoft Hyper-V, Virtual Server, Virtual PC	& 00-03-FF \\
			Parallells Desktop, Workstation, Server, Virtuozzo & 00-1C-42 \\
			Virtual Iron 4 & 00-0F-4B \\
			Red Hat Xen	& 00-16-3E \\
			Oracle VM	& 00-16-3E \\
			XenSource	& 00-16-3E \\
			Novell Xen & 00-16-3E \\
			Sun xVM VirtualBox & 08-00-27 \\
		\hline
		\end{tabular}
	\caption{First three octets of default MAC addresses by virtualization platform \emph{(Source:http://www.techrepublic.com/blog/networking/mac-address-scorecard-for-common-virtual-machine-platforms/538)}}
	\label{tab:macaddrs}
\end{table*}


\subsection{Identifying NATed Host by Bandwidth Profiles}
\label{subsec:BWprofiles}	

\begin{figure}[htb]
		\includegraphics[scale=0.34]{timing_small.png}
	\caption{When reading small files, bandwidth profiles are not yet clearly ordered.}
	\label{fig:timing_small}
\end{figure}

\begin{figure}[htb]
		\includegraphics[scale=0.3]{timing_large.png}
	\caption{When reading larger files, the bandwidth profiles are clearly differentiated.}
	\label{fig:timing_large}
\end{figure}

The MAC address distinction does not help us in the case of VM-based clients that are using user-mode (NAT-based) networking instead of bridging, as NATed guests are not visible to the rest of the network and do not have an externally identifiable IP and MAC address. As discussed above, all guest VM traffic is rewritten inside the host to contain the MAC and IP addresses of the host NIC. However, user-mode networking suffers from a substantial slowdown compared to physical or bridged hosts. In a controlled environment, an NFS server can take advantage of this dramatically decreased bandwidth relative to a native-client baseline to conclude that the client is virtualized. 

Figures \ref{fig:timing_small} and \ref{fig:timing_large} show the results of timing tests in which the client VMs under various networking configurations read 500KB, 1MB, 500MB, 1GB, and 2GB files from an NFS server according to the configuration describe in Subsection \ref{subsec:expSetup}. Files were read in 4KB blocks, and file buffer caches were cleared between tests by running \texttt{echo 3 > /proc/sys/vm/drop\_caches}. Figure \ref{fig:timing_small} shows that when reading smaller files, the bandwidths of the various client-VM networking configurations have not yet achieved a stable relative ordering. However, when reading larger files, as shown in Figure \ref{fig:timing_large}, a clear relative bandwidth ordering emerges.

Not surprisingly, reads on the server itself (directly from the local ext4 file system) were fastest and were mirrored almost exactly by reads from the NFS client running on native hardware. The reads performed locally on the server and from the native client differed by only 43 milliseconds across all tests, so their lines are coincident in Figure \ref{fig:timing_large}. Of the virtualized clients, using the paravirtualized virtio driver under bridged networking imposed only a slight overhead relative to the native-client baseline. However, switching to either the fully emulated RealTek RTL8139 virtual network driver or using NATed instead of bridged networking imposed a substantial bandwidth penalty. Doing both (using the RTL8139 under user-mode networking) results in 10.7x slowdown relative to our baseline. 

Table \ref{tab:NFSBandwidthProfiles} shows the bandwidths observed across the different network configurations of our client KVM+QEMU VM when performing the NFS read-timing tests. Importantly, the bandwidths are highly differentiated. In a controlled environment, this would allow an NFS server to take a ``bandwidth fingerprint'' of an NFS client by performing a sustained read and recording the bandwidth, and infer from its bandwidth whether the client was running on a native or virtualized platform. 

\begin{table}[htpb]
		\begin{tabular}{|l|l|}
			\hline
			\textbf{Client Network Stack} & \textbf{Bandwidth} (MB/sec) \\
			\hline
			Native & 48.3 \\
			Virtio Bridged & 28.3 \\
			Virtio NAT & 11.9 \\
			RealTek Bridged & 10.2 \\
			RealTek NAT & 4.6 \\
			\hline
		\end{tabular}
	\caption{NFS Bandwidth Profiles}
	\label{tab:NFSBandwidthProfiles}
\end{table}

\subsection{NFS Client Identification Conclusions}
Given a lack of signals at the application layer that might differentiate native and virtual NFS clients, the two heuristics described above provide an NFS server with a rough means of separating the two classes of clients. When combined, the heuristics should provide a good indication of a client's underlying platform. If the client is using bridged networking under a default configuration, their bandwidth may be close to the native-client baseline but their MAC address will identify them as virtualized with a high degree of certainty. On the other hand, if the client is using user-mode networking, they will not be identifiable based on their MAC address, but their substantially decreased bandwidth can suggest their use of user-mode networking. 

In the next section, we shift gears to try to isolate the origin of the decreased bandwidth of user-mode networking from the client side.

\input{trace}
%\subsection{RealTek 8139 NAT: Guest Transmit}
%User-mode networking under QEMU uses network address translation (NAT) in order to allow a guest VM to establish connections with outside networks. QEMU performs address translation using a protocol from the 1990s known as SLiRP, which was originally used to provide TCP/IP access to dial-up shell account users by emulating the TCP/IP stack. Though that usage is largely obsolete, the same address translation techniques are applied in QEMU user-mode networking to provide NAT functionality.

\section{Future Work}
Our initial cross-comparison of virtual network drivers and networking configurations could be expanded in future work to encompass the full range of drivers available under KVM+QEMU. This would contribute to a fuller understanding of the networking landscape under KVM+QEMU, and show which options and combinations are the most efficient under varying workloads. 

While it is clear that KVM+QEMU users searching for optimal network performance should use virtio under bridged networking, that option may not be available to all users. Future work might also look at providing a better out-of-the-box default option for user-mode networking, perhaps by performing code-path and memory-management optimizations in the NAT code path between the guest and host network stacks.

\section{Conclusions}
We began by searching for methods by which an NFS server could differentiate native from virtualized clients, and developed two heuristics for this purpose. First, bridged clients can be detected by matching the first three octets of their MAC address against a list of default MAC address prefixes used by virtualization platforms for their bridged guest VMs. If a match is found, this indicates the client is virtualized and is using the corresponding platform. However, false negatives are possible if the client or administrator proactively sets a different MAC address. 

Our second method assumes a controlled or predictable network environment and uses bandwidth profiles to distinguish virtual clients against a baseline set by accesses from a native NFS client. It takes advantage of our observation that virtualized clients, especially when using user-mode networking and the fully emulated RTL8139 driver, experience as much as 10.7x drop in bandwidth relative to the native-client baseline. 

This led us to investigate the source of the poor performance under user-mode networking, which we determined was primarily the result of the extensive memory management and packet manipulation inherent in the NAT process as the packet travels between guest and host network stacks.

%Our initial cross-comparison of virtual network drivers and networking configurations could be expanded in future work to encompass the full range of drivers available under KVM+QEMU. This would contribute to a fuller understanding of the networking landscape under KVM+QEMU, and show which options and combinations are the most efficient under varying workloads. 

%While it is clear that KVM+QEMU users searching for optimal network performance should use virtio under bridged networking, that option may not be available to all users. Future work might also look at providing a better out-of-the-box default option for user-mode networking, perhaps by performing code-path and memory-management optimizations in the NAT code path between the guest and host network stacks.

{\footnotesize \bibliographystyle{acm}
\bibliography{references}}

\section*{Appendix}
\label{sec:appendix}
\subsection*{Shared Code Path: Transmit Side}
Here we provide a brief outline of the code path shared by both bridged and user-mode networking on guest transmissions as a packet travels from the bottom of the guest network stack and moves toward either \texttt{tap\_receive} or \texttt{net\_slirp\_receive}. 

\begin{verbatim}
rtl8139_transfer_frame (end guest 
 |                      network stack)
 |
 -> qemu_send_packet
 |
 -> qemu_send_packet_async
 |
 -> qemu_send_packet_async_with_flags
 |
 -> qemu_net_queue_send
 |
 -> qemu_net_queue_deliver
 |
 -> qemu_deliver_packet
 |
 -> net_hub_port_receive
 |
 -> net_hub_receive
 |
 -> qemu_send_packet' (now it's running 
 |                     through different 
 |                     interfaces: figured 
 |                     out that it needs  
 |                     to be moving toward 
 |                     the TAP device.)
 |
 -> qemu_send_packet_async'
 |
 -> qemu_send_packet_async_with_flags'
 |
 -> qemu_net_queue_deliver'
 |
 -> qemu_deliver_packet'
 |
 -> +-> tap_receive (bridged)
    |
    |
    -> net_slirp_receive (NAT)
       |
       -> slirp_input 
\end{verbatim}

Note that the second run of each function (indicated as \texttt{function'}) performs different tasks than the first run, as they are performing operations using different function pointers. At a high level, this shared code processes packets originating from the guest network stack and forwards them toward the correct intermediate code depending on whether the guest VM is using bridged or NATed networking.

\subsection*{Code Path: Receive Side}
These functions bridge the gap between the top of the host network stack and pass the incoming packets to the bottom of the guest network stack---in our case, this is to the RTL8139 emulated network driver. The functions encountered at the start of the shared receive-side code below, \texttt{qemu\_send\_packet\_*}, forward the incoming packets to guest network stack based on their incoming queue.

\subsubsection*{Bridged}
The functions in the bridged receive-side code do not touch the packet headers at all---they just pass the packets straight up to the guest from the physical interface based on their incoming queue. Thus, the packets have come directly from the physical NIC and don't need to have their headers retranslated.

\begin{verbatim}
main_loop_wait
 |
 -> qemu_io_handler_poll
 |
 -> tap_send
 |
 -> tap_read_packet
 |
 -> qemu_send_packet
 |
 -> qemu_send_packet_async
 |
 -> ... continue to shared code. 
\end{verbatim}

\subsubsection*{NAT}
In the NAT receive-side code, all the fields in the arriving packets' headers are in host byte order because they have passed through the full host network stack and are coming off the application layer of the host network stack. So at this stage everything gets ripped off and translated back to network byte order and passed to the RTL8139 network driver of the guest stack, which once again decapsulates the packets for the guest application layer. So the flow is: decapsulate the packet in host network stack; re-encapsulate it here and pass it to bottom of the guest network stack; the guest network stack again decapsulates the packet (essentially repeating the work of the host network stack, but with the headers that were modified by NAT). 

\begin{verbatim}
main_loop_wait
 |
 -> slirp_select_poll
 |
 -> tcp_output -- strips the headers
 |
 -> ip_output -- gets new headers based 
 |
 -> if_output
 |
 -> if_start
 |
 -> if_encap
 |
 -> slirp_output
 |
 -> qemu_send_packet
 |
 -> qemu_send_packet_async
  |
 -> ... continue to shared code.
\end{verbatim}

\end{document}
