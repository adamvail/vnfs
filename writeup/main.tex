\documentclass[11pt,pdftex,twocolumn]{article}
\usepackage{alltt}
\usepackage[dvips]{graphicx}
\usepackage{verbatim}
\usepackage[margin=1in, paperwidth=8.5in, paperheight=11in]{geometry}
\usepackage{url}

\title{Server Side NFS Identification and Client-Side Packet Tracing in a Virtualized Environment}
\author{Rob Jellinek and Adam Vail}
\begin{document}
\maketitle

\begin{abstract}
Need to do the abstract
\end{abstract}
\input{intro}
We start with NFS: asking the question, how can an NFS server differentiate requests coming from physical machines vs. virtual machines--and in particular, from multiple virtual machines on the same physical machine. Knowing this could allow the server to coalesce packets it's sending if those packets are all going to the same physical machine. 

From what we were able to determine, the NFS protocol, being an application-layer protocol, is indistinguishable in the virtual and native settings. Without explicit signals built into the NFS protocol, nothing from the application layer NFS protocol signals the origin and platform of the client. 

We learned that the physical server can determine whether requests are coming from bridged VMs by detecting a set of standard MAC addresses assigned by the various virtual machine platforms. The caveat is that the user can always set/spoof their own MAC address. In this case, if you're bridged and have spoofed a legitimate MAC address, the VM's traffic is indistinguishable from traffic coming from a physical device, both in terms of the MAC address, and in terms of timing.

If the client is NATed, then all packet header information in the first three layers of all client-VM outgoing packets get remapped to the physical machine's headers. In this case, there is a dramatic slowdown due to the time it takes the NAT code to map packets from the guest network stack to the host network stack. The remainder of our work focused on determining why and how user-mode networking suffers such a dramatic slowdown compared to bridged networking, and in particular, what code path in the NATed environment causes this slowdown.


%\input{motivation}

\input{design}
Talk about our system setup here. 

\section{NFS}
We began our quest by searching for a way to differentiate NFS requests from physical and virtual clients, from the standpoint of an NFS server. If a server could distinguish the two clients--and moreover, if it could identify the single physical machine from which multiple VM-accesses originated--then there is the potential to coalesce NFS server responses to multiple VMs on the same physical machine. 

We investigated this on three fronts. First, we searched through the NFS 4.0 RFC specification to try to find operations that could potentially differ in the VM or physical context, or otherwise somehow uniquely mark the origin of the client machine. Second, we performed a number of tcpdump traces on NFS 4.0 \texttt{mount} operations as well as file transfers using \texttt{cat}, with the goal of isolating differences in the action of the NFS protocol in the virtual and physical settings. The tcpdump traces were executed with the flags \texttt{tcpdump -vvnnXSs 1536} in order to examine the full contents of each NFS packet exchanged between the client and server. Finally, we timed NFS reads of varying sizes in the virtual and physical settings in order to determine if timing differences could differentiate physical from virtual clients. 

We learned several things from these investigations. First, we were unable to differentiate any aspect of the NFS protocol in the virtual setting from its behavior in the physical setting at the application layer, either in our reading of the NFS 4.0 RFC or in our tcpdump traces. This is not surprising, as there is nothing that should inherently differ about the behavior of the application layer in a virtual machine. Looking lower in the network stack, however, we realized that the MAC addresses assigned to the virtual NICs of bridged connections took on default values under all the virtualization platforms we investigated, allowing us to distinguish the VM hypervisor in these cases. We discuss this in more detail in Section \ref{subsec:macaddrs} below. 

\subsection{Identifying Bridged Hosts by MAC Address}
\label{subsec:macaddrs}



\section{Bridged vs. NAT Timing}
Bridged vs. NAT

graph with iperf results: show it's a big difference.

RTL8139 (KVM's default)

Answer: why is Virtio faster than the fully emulated RTL8139

\section{Following the Code Path}
In order to determine where the bottlenecks are occurring using the RTL8139 driver which performs full emulation, we traced the code path of a packet from the guest's application layer through the guest and host network stacks. We provide an explanation of the traces and relevant functions for the code path for the transmit side below, and an outline of the path for the receive side in the Appendix, along with the code path followed by packets on the transmit side that are shared by both the NAT and bridged code. 


\subsection{RealTek 8139 NAT: Guest Transmit}
User-mode networking under QEMU uses network address translation (NAT) in order to allow a guest VM to establish connections with outside networks. QEMU performs address translation using a protocol from the 1990s known as SLIRP, which was originally used to provide TCP/IP access to dial-up shell account users by emulating the TCP/IP stack. Though that usage is largely obsolete, the same address translation techniques are applied in QEMU user-mode networking 


slirp_input:
|          |
|           -> m_get (allocates buffer space for packet to be copied into)
|               |
|                -> slirp_insque (manages doubly linked list of buffer space that is being used for the packets)
|
 ->ip_input (converts ip fields to host representation using ntohs, manipulating buffer space that was created in m_get)
    |
    |
     -> tcp_input (converts tcp fields to host format. Rewrites all the fields to host representation)
         |
         |
          -> tcp_output (standard tcp things happen here. e.g. calculating receive window. There is a lot going on here. Flips tcp fields back from host byte order to network byte order)
              |
              |
               -> ip_output (flips ip fields back to network byte order from host byte order)
                   |
                   |
                    -> if_output 
                        |     + Figures out which queue to place the packet on
                        |         + fastq: meant for packets associated with interactive activities. Determined through the IPTOS_NODELAY flag set in the ip header of the packet
                        |         + batchq: queue for non-interactive actions. This is a more efficient queue.
                        |     + Favorite comment:
                        |           "These are arbitrary numbers, probably not optimal"
                        |     + creates a new doubly linked list through m_get, not entirely clear why they need it, it looks like another packet queue
                        |
                         -> if_start
                             |    + Drains the fastq before the batchq
                             |    + cleans up the buffers and frees memory after if_encap returns
                             |      (i.e., it calls slirp_remque and m_free, which cleans up the buffers)
                             |
                              -> if_encap
                                  |    + Copies packet headers into appropriate header structs, essentially preparing the packet
                                  |
                                   -> slirp_output
                                       |         + Add slirp state info and pass the packet off to qemu to get it out to the wire
                                       |
                                        -> qemu_send_packet
                                                       + Passes packet through several wrapper functions which finally ends with qemu_net_queue_append which adds the packet to a queue which I assume puts it on the wire.
                                                        + wrapper for qemu_send_packet_async with the last argument set to NULL

\subsection{RealTek 8139 Bridged: Guest Transmit}
We first come out the end of the shared code (see Appendix) and get to tap_receive. The flow continues as follows:

tap_receive
tap_receive_raw
tap_write_packet
writev 


%\input{implementation}
%\input{evaluation}
%\input{future}
%\input{related}
%\input{conclusion}

\section{Appendix}

[TODO: provide a graph of the code that happens whether you're shared or bridged on a guest transmit]
The code is implementing a deisgn pattern through the following functions: 

GUEST NETWORK STACK finishes here
--- start QEMU user space emulating physical NIC here ---
rtl8139_transfer_frame 
qemu_send_packet
qemu_send_packet_async
qemu_send_packet_async_with_flags
qemu_net_queue_send
qemu_net_queue_deliver
qemu_deliver_packet
net_hub_port_receive
net_hub_receive
qemu_send_packet' (now it's running through different interfaces: figured out that it needs to be moving toward the TAP device.)
qemu_send_packet_async'
qemu_send_packet_async_with_flags'
qemu_net_queue_deliver'
qemu_deliver_packet'
tap_receive / net_slirp_receive depending on whether you're using bridged or NAT.

This is shared by everyone. It's connecting the driver at the top to the actual networking at the bottom. 

------

RECEIVE SIDE:
These functions bridge the gap between the top of the host network stack and pass the incoming packets to the bottom of the guest network stack--the RTL8139 emulated network driver. 

\header{Bridged}
Here tap_send, etc., don't mess with the packet headers at all--they just pass the packets straight up to the guest from the physical interface, possibly via queue using qemu_notify--so the packets have come directly from the physical NIC and don't need to have their headers retranslated.

main_loop_wait
qemu_io_handler_poll
tap_send
tap_read_packet
if the size read > 0:
	qemu_send_packet_async
... continue to shared code. The interested reader can check out the KVM source, wazzup.

\header{NAT}
Everything is in host byte order, because it's passed through the full host network stack and is coming off the application layer of the host network stack. So at this stage everything gets ripped off and translated back to network byte order and passed to the RTL8139 network driver of the guest stack, which the once again decapsulates the packets for the guest application layer. So the flow is: decapsulate in host network stack; re-encapsulate here and pass to bottom of guest network stack; guest network stack re-decapsulates packet (essentially repeating the work of the host network stack, but with the headers that were modified by NATing). 

main_loop_wait
slirp_select_poll
tcp_output -- strips the headers
ip_output -- gets new headers based 
if_output
if_start
if_encap
slirp_output

This could be why we're getting weird iperf data--because 








{\footnotesize \bibliographystyle{acm}
\bibliography{references}}



\end{document}
